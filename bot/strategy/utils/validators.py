# bot/strategy/utils/validators.py
"""
–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
from datetime import datetime, timezone, timedelta
import logging
from dataclasses import dataclass
from enum import Enum

from ..base.enums import ValidationLevel, TimeFrame

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)


class ValidationError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    pass


class DataQuality(Enum):
    """–£—Ä–æ–≤–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    EXCELLENT = "excellent"
    GOOD = "good"  
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    CRITICAL = "critical"


@dataclass
class ValidationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    is_valid: bool
    quality: DataQuality
    errors: List[str]
    warnings: List[str]
    data_points: int
    missing_data_pct: float
    time_coverage: Optional[timedelta]
    recommendations: List[str]
    
    @property
    def has_errors(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ—à–∏–±–æ–∫"""
        return len(self.errors) > 0
    
    @property  
    def has_warnings(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π"""
        return len(self.warnings) > 0
    
    @property
    def is_usable(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–≥–æ–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        return self.is_valid and self.quality != DataQuality.CRITICAL
    
    def __str__(self) -> str:
        status = "‚úÖ –í–ê–õ–ò–î–ù–´" if self.is_valid else "‚ùå –ù–ï –í–ê–õ–ò–î–ù–´"
        return f"–î–∞–Ω–Ω—ã–µ {status} | –ö–∞—á–µ—Å—Ç–≤–æ: {self.quality.value} | –¢–æ—á–µ–∫: {self.data_points}"


class DataValidator:
    """
    –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ —Å—Ç—Ä–æ–≥–æ—Å—Ç–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    
    # –¢—Ä–µ–±—É–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
    OHLC_COLUMNS = ['open', 'high', 'low', 'close']
    OHLCV_COLUMNS = OHLC_COLUMNS + ['volume']
    DELTA_COLUMNS = OHLCV_COLUMNS + ['delta']
    EXTENDED_COLUMNS = OHLCV_COLUMNS + ['buy_volume', 'sell_volume']
    
    @staticmethod
    def validate_basic_data(df: pd.DataFrame, 
                          validation_level: ValidationLevel = ValidationLevel.STANDARD,
                          required_columns: Optional[List[str]] = None) -> ValidationResult:
        """
        –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è OHLCV –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            df: DataFrame –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            validation_level: –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–æ–≥–æ—Å—Ç–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            required_columns: –°–ø–∏—Å–æ–∫ —Ç—Ä–µ–±—É–µ–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é OHLCV)
        
        Returns:
            ValidationResult —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        errors = []
        warnings = []
        recommendations = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º OHLCV –∫–æ–ª–æ–Ω–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if required_columns is None:
            required_columns = DataValidator.OHLCV_COLUMNS
        
        try:
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π DataFrame
            if df is None:
                errors.append("DataFrame —Ä–∞–≤–µ–Ω None")
                return ValidationResult(
                    is_valid=False, quality=DataQuality.CRITICAL, 
                    errors=errors, warnings=warnings, data_points=0,
                    missing_data_pct=100.0, time_coverage=None,
                    recommendations=["–ü—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"]
                )
            
            if df.empty:
                errors.append("DataFrame –ø—É—Å—Ç")
                return ValidationResult(
                    is_valid=False, quality=DataQuality.CRITICAL,
                    errors=errors, warnings=warnings, data_points=0,
                    missing_data_pct=100.0, time_coverage=None,
                    recommendations=["–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"]
                )
            
            data_points = len(df)
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
            min_required = validation_level.min_data_points
            if data_points < min_required:
                errors.append(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {data_points} < {min_required}")
            
            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ç—Ä–µ–±—É–µ–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
                return ValidationResult(
                    is_valid=False, quality=DataQuality.CRITICAL,
                    errors=errors, warnings=warnings, data_points=data_points,
                    missing_data_pct=100.0, time_coverage=None,
                    recommendations=[f"–î–æ–±–∞–≤—å—Ç–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}"]
                )
            
            # 4. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤
            numeric_errors = []
            for col in required_columns:
                if col in df.columns:
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    except Exception as e:
                        numeric_errors.append(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ {col}: {e}")
            
            if numeric_errors:
                errors.extend(numeric_errors)
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∑–Ω–∞—á–µ–Ω–∏—è
            nan_check = df[required_columns].isnull()
            total_nan = nan_check.sum().sum()
            total_values = len(df) * len(required_columns)
            missing_data_pct = (total_nan / total_values) * 100
            
            if validation_level == ValidationLevel.PARANOID and total_nan > 0:
                errors.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è: {total_nan}")
            elif validation_level == ValidationLevel.STRICT and missing_data_pct > 1:
                errors.append(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ NaN –∑–Ω–∞—á–µ–Ω–∏–π: {missing_data_pct:.1f}%")
            elif missing_data_pct > 5:
                warnings.append(f"–ú–Ω–æ–≥–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {missing_data_pct:.1f}%")
            
            # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏—á–Ω–æ—Å—Ç–∏ OHLC –¥–∞–Ω–Ω—ã—Ö
            if all(col in df.columns for col in DataValidator.OHLC_COLUMNS):
                ohlc_issues = DataValidator._validate_ohlc_logic(df)
                if ohlc_issues:
                    if validation_level in [ValidationLevel.STRICT, ValidationLevel.PARANOID]:
                        errors.extend(ohlc_issues)
                    else:
                        warnings.extend(ohlc_issues)
            
            # 7. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω—É–ª–µ–≤—ã–µ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
            price_columns = [col for col in DataValidator.OHLC_COLUMNS if col in df.columns]
            invalid_prices = (df[price_columns] <= 0).any().any()
            if invalid_prices:
                errors.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω—É–ª–µ–≤—ã–µ –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã")
            
            # 8. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—ä–µ–º–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if 'volume' in df.columns:
                negative_volume = (df['volume'] < 0).any()
                if negative_volume:
                    errors.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –æ–±—ä–µ–º—ã")
                
                zero_volume_pct = (df['volume'] == 0).sum() / len(df) * 100
                if zero_volume_pct > 10:
                    warnings.append(f"–ú–Ω–æ–≥–æ –Ω—É–ª–µ–≤—ã—Ö –æ–±—ä–µ–º–æ–≤: {zero_volume_pct:.1f}%")
            
            # 9. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ (–µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —è–≤–ª—è–µ—Ç—Å—è datetime)
            time_coverage = None
            if isinstance(df.index, pd.DatetimeIndex):
                time_issues = DataValidator._validate_time_series(df, validation_level)
                warnings.extend(time_issues)
                
                if len(df) > 1:
                    time_coverage = df.index[-1] - df.index[0]
            
            # 10. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã (–¥–ª—è —Å—Ç—Ä–æ–≥–∏—Ö —É—Ä–æ–≤–Ω–µ–π)
            if validation_level in [ValidationLevel.STRICT, ValidationLevel.PARANOID]:
                outlier_issues = DataValidator._detect_outliers(df, required_columns)
                if outlier_issues:
                    warnings.extend(outlier_issues)
            
            # 11. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
            quality = DataValidator._determine_data_quality(
                len(errors), len(warnings), missing_data_pct, data_points, min_required
            )
            
            # 12. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            recommendations = DataValidator._generate_recommendations(
                quality, missing_data_pct, data_points, min_required, validation_level
            )
            
            # –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            is_valid = len(errors) == 0 and quality != DataQuality.CRITICAL
            
            return ValidationResult(
                is_valid=is_valid,
                quality=quality,
                errors=errors,
                warnings=warnings,
                data_points=data_points,
                missing_data_pct=missing_data_pct,
                time_coverage=time_coverage,
                recommendations=recommendations
            )
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return ValidationResult(
                is_valid=False,
                quality=DataQuality.CRITICAL,
                errors=[f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}"],
                warnings=[],
                data_points=len(df) if df is not None else 0,
                missing_data_pct=100.0,
                time_coverage=None,
                recommendations=["–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö"]
            )
    
    @staticmethod
    def _validate_ohlc_logic(df: pd.DataFrame) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏—á–Ω–æ—Å—Ç–∏ OHLC –¥–∞–Ω–Ω—ã—Ö"""
        issues = []
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ high >= low
            high_low_invalid = (df['low'] > df['high']).any()
            if high_low_invalid:
                issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –±–∞—Ä—ã –≥–¥–µ low > high")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ open –∏ close –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ high-low  
            open_invalid = ((df['open'] > df['high']) | (df['open'] < df['low'])).any()
            if open_invalid:
                issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –±–∞—Ä—ã –≥–¥–µ open –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ high-low")
            
            close_invalid = ((df['close'] > df['high']) | (df['close'] < df['low'])).any()
            if close_invalid:
                issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –±–∞—Ä—ã –≥–¥–µ close –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ high-low")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
            ranges = (df['high'] - df['low']) / df['close']
            large_ranges = (ranges > 0.1).sum()  # –î–∏–∞–ø–∞–∑–æ–Ω –±–æ–ª—å—à–µ 10%
            if large_ranges > len(df) * 0.05:  # –ë–æ–ª—å—à–µ 5% –æ—Ç –≤—Å–µ—Ö –±–∞—Ä–æ–≤
                issues.append(f"–ú–Ω–æ–≥–æ –±–∞—Ä–æ–≤ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏: {large_ranges}")
                
        except Exception as e:
            issues.append(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ OHLC –ª–æ–≥–∏–∫–∏: {e}")
        
        return issues
    
    @staticmethod
    def _validate_time_series(df: pd.DataFrame, validation_level: ValidationLevel) -> List[str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"""
        issues = []
        
        try:
            if not isinstance(df.index, pd.DatetimeIndex):
                return ["–ò–Ω–¥–µ–∫—Å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è DatetimeIndex"]
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            duplicates = df.index.duplicated().sum()
            if duplicates > 0:
                issues.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏: {duplicates}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            if not df.index.is_monotonic_increasing:
                issues.append("–î–∞–Ω–Ω—ã–µ –Ω–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–ª—è —Å—Ç—Ä–æ–≥–∏—Ö —É—Ä–æ–≤–Ω–µ–π)
            if validation_level in [ValidationLevel.STRICT, ValidationLevel.PARANOID] and len(df) > 2:
                time_diffs = df.index.to_series().diff().dropna()
                median_diff = time_diffs.median()
                
                # –ù–∞—Ö–æ–¥–∏–º –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏ (–±–æ–ª—å—à–µ —á–µ–º 2x –º–µ–¥–∏–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª)
                large_gaps = (time_diffs > median_diff * 2).sum()
                if large_gaps > 0:
                    issues.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏: {large_gaps}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
            if len(df) > 0:
                last_time = df.index[-1]
                if pd.Timestamp.now(tz=timezone.utc) - last_time > timedelta(hours=24):
                    issues.append("–î–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–º–∏ (—Å—Ç–∞—Ä—à–µ 24 —á–∞—Å–æ–≤)")
                    
        except Exception as e:
            issues.append(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤: {e}")
        
        return issues
    
    @staticmethod
    def _detect_outliers(df: pd.DataFrame, columns: List[str]) -> List[str]:
        """–î–µ—Ç–µ–∫—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö"""
        issues = []
        
        try:
            for col in columns:
                if col not in df.columns:
                    continue
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º IQR –º–µ—Ç–æ–¥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤—ã–±—Ä–æ—Å–æ–≤
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
                outlier_pct = outliers / len(df) * 100
                
                if outlier_pct > 5:  # –ë–æ–ª—å—à–µ 5% –≤—ã–±—Ä–æ—Å–æ–≤
                    issues.append(f"–ú–Ω–æ–≥–æ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –∫–æ–ª–æ–Ω–∫–µ {col}: {outlier_pct:.1f}%")
                    
        except Exception as e:
            issues.append(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤—ã–±—Ä–æ—Å–æ–≤: {e}")
        
        return issues
    
    @staticmethod
    def _determine_data_quality(errors_count: int, warnings_count: int, 
                              missing_pct: float, data_points: int, min_required: int) -> DataQuality:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        if errors_count > 0:
            return DataQuality.CRITICAL
        
        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        quality_score = 100
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        quality_score -= warnings_count * 5
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if missing_pct > 10:
            quality_score -= 30
        elif missing_pct > 5:
            quality_score -= 15
        elif missing_pct > 1:
            quality_score -= 5
        
        # –®—Ç—Ä–∞—Ñ—ã –∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö
        data_ratio = data_points / min_required
        if data_ratio < 1.5:
            quality_score -= 20
        elif data_ratio < 2:
            quality_score -= 10
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É —Å—á–µ—Ç—É
        if quality_score >= 90:
            return DataQuality.EXCELLENT
        elif quality_score >= 75:
            return DataQuality.GOOD
        elif quality_score >= 60:
            return DataQuality.ACCEPTABLE
        elif quality_score >= 40:
            return DataQuality.POOR
        else:
            return DataQuality.CRITICAL
    
    @staticmethod
    def _generate_recommendations(quality: DataQuality, missing_pct: float, 
                                data_points: int, min_required: int,
                                validation_level: ValidationLevel) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –¥–∞–Ω–Ω—ã—Ö"""
        recommendations = []
        
        if quality == DataQuality.CRITICAL:
            recommendations.append("üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ")
            recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞")
        
        if data_points < min_required:
            needed = min_required - data_points
            recommendations.append(f"üìä –ó–∞–≥—Ä—É–∑–∏—Ç–µ –µ—â–µ {needed} —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö")
        
        if missing_pct > 10:
            recommendations.append("üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö - —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤")
        elif missing_pct > 5:
            recommendations.append("‚ö†Ô∏è –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
        
        if quality == DataQuality.POOR:
            recommendations.append("üìà –£–≤–µ–ª–∏—á—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º")
            recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        
        if validation_level == ValidationLevel.BASIC and quality in [DataQuality.GOOD, DataQuality.EXCELLENT]:
            recommendations.append("‚ú® –î–∞–Ω–Ω—ã–µ —Ö–æ—Ä–æ—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å")
        
        if not recommendations:
            recommendations.append("‚úÖ –î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
        
        return recommendations


class MultiTimeframeValidator:
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–ª—è –º—É–ª—å—Ç–∏—Ç–∞–π–º—Ñ—Ä–µ–π–º –¥–∞–Ω–Ω—ã—Ö"""
    
    @staticmethod
    def validate_multitimeframe_data(data: Dict[str, pd.DataFrame], 
                                   required_timeframes: List[TimeFrame],
                                   validation_level: ValidationLevel = ValidationLevel.STANDARD) -> Dict[str, ValidationResult]:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
        
        Args:
            data: –°–ª–æ–≤–∞—Ä—å {timeframe: DataFrame}
            required_timeframes: –°–ø–∏—Å–æ–∫ —Ç—Ä–µ–±—É–µ–º—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
            validation_level: –£—Ä–æ–≤–µ–Ω—å –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¢–§
        """
        results = {}
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
        for tf in required_timeframes:
            tf_key = tf.value
            
            if tf_key not in data:
                results[tf_key] = ValidationResult(
                    is_valid=False,
                    quality=DataQuality.CRITICAL,
                    errors=[f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞ {tf_key}"],
                    warnings=[],
                    data_points=0,
                    missing_data_pct=100.0,
                    time_coverage=None,
                    recommendations=[f"–ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {tf_key}"]
                )
                continue
            
            # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¢–§
            result = DataValidator.validate_basic_data(data[tf_key], validation_level)
            results[tf_key] = result
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –º–µ–∂–¥—É –¢–§
        sync_issues = MultiTimeframeValidator._check_timeframe_synchronization(data, required_timeframes)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        if sync_issues:
            for tf_key in results:
                if results[tf_key].is_valid:
                    results[tf_key].warnings.extend(sync_issues)
        
        return results
    
    @staticmethod
    def _check_timeframe_synchronization(data: Dict[str, pd.DataFrame], 
                                       timeframes: List[TimeFrame]) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –º–µ–∂–¥—É —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞–º–∏"""
        issues = []
        
        try:
            available_data = {tf.value: data[tf.value] for tf in timeframes if tf.value in data}
            
            if len(available_data) < 2:
                return issues  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 2 –¢–§ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            last_times = {}
            for tf_key, df in available_data.items():
                if isinstance(df.index, pd.DatetimeIndex) and len(df) > 0:
                    last_times[tf_key] = df.index[-1]
            
            if len(last_times) >= 2:
                times = list(last_times.values())
                max_time = max(times)
                min_time = min(times)
                time_diff = (max_time - min_time).total_seconds()
                
                # –î–æ–ø—É—Å—Ç–∏–º–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
                max_allowed_diff = max([tf.seconds for tf in timeframes]) * 2
                
                if time_diff > max_allowed_diff:
                    issues.append(f"–ë–æ–ª—å—à–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –º–µ–∂–¥—É –¢–§: {time_diff}s")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö
            data_coverage = {}
            for tf_key, df in available_data.items():
                if isinstance(df.index, pd.DatetimeIndex) and len(df) > 1:
                    coverage = df.index[-1] - df.index[0]
                    data_coverage[tf_key] = coverage
            
            if data_coverage:
                coverages = list(data_coverage.values())
                min_coverage = min(coverages)
                max_coverage = max(coverages)
                
                # –ï—Å–ª–∏ —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –ø–æ–∫—Ä—ã—Ç–∏–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è
                if max_coverage > min_coverage * 2:
                    issues.append("–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –ø–æ–∫—Ä—ã—Ç–∏–∏ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –¢–§")
                    
        except Exception as e:
            issues.append(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¢–§: {e}")
        
        return issues


class StrategyDataValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
    
    @staticmethod
    def validate_volume_vwap_data(df: pd.DataFrame, 
                                 validation_level: ValidationLevel = ValidationLevel.STANDARD) -> ValidationResult:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è VWAP —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—ä–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        required_columns = DataValidator.OHLCV_COLUMNS
        result = DataValidator.validate_basic_data(df, validation_level, required_columns)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è VWAP
        if result.is_valid and 'volume' in df.columns:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—ä–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            zero_volume_count = (df['volume'] == 0).sum()
            if zero_volume_count > len(df) * 0.1:  # –ë–æ–ª—å—à–µ 10% –Ω—É–ª–µ–≤—ã—Ö –æ–±—ä–µ–º–æ–≤
                result.warnings.append(f"–ú–Ω–æ–≥–æ –Ω—É–ª–µ–≤—ã—Ö –æ–±—ä–µ–º–æ–≤: {zero_volume_count}")
                result.recommendations.append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –æ–±—ä–µ–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        return result
    
    @staticmethod
    def validate_cumdelta_data(df: pd.DataFrame,
                              validation_level: ValidationLevel = ValidationLevel.STANDARD) -> ValidationResult:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CumDelta —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –¥–µ–ª—å—Ç–µ
        has_delta_columns = any(col in df.columns for col in ['delta', 'buy_volume', 'sell_volume'])
        
        if has_delta_columns:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            available_cols = [col for col in ['delta', 'buy_volume', 'sell_volume'] if col in df.columns]
            required_columns = DataValidator.OHLCV_COLUMNS + available_cols
        else:
            # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ OHLCV
            required_columns = DataValidator.OHLCV_COLUMNS
        
        result = DataValidator.validate_basic_data(df, validation_level, required_columns)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –¥–µ–ª—å—Ç—ã
        if result.is_valid:
            if not has_delta_columns:
                result.warnings.append("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –¥–µ–ª—å—Ç–µ - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback —Ä–∞—Å—á–µ—Ç")
                result.recommendations.append("–î–æ–±–∞–≤—å—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ delta –∏–ª–∏ buy_volume/sell_volume –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏")
        
        return result
    
    @staticmethod
    def validate_multitf_data(data: Dict[str, pd.DataFrame], 
                             fast_tf: TimeFrame, slow_tf: TimeFrame,
                             validation_level: ValidationLevel = ValidationLevel.STANDARD) -> Dict[str, ValidationResult]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Multi-timeframe —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        return MultiTimeframeValidator.validate_multitimeframe_data(
            data, [fast_tf, slow_tf], validation_level
        )


# =========================================================================
# –£–¢–ò–õ–ò–¢–ù–´–ï –§–£–ù–ö–¶–ò–ò
# =========================================================================

def quick_validate(df: pd.DataFrame, strategy_type: str = "basic") -> bool:
    """
    –ë—ã—Å—Ç—Ä–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏)
    
    Args:
        df: DataFrame –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        strategy_type: –¢–∏–ø —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ ("basic", "volume_vwap", "cumdelta", "multitf")
    
    Returns:
        bool: True –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–≥–æ–¥–Ω—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
    """
    try:
        if strategy_type == "volume_vwap":
            result = StrategyDataValidator.validate_volume_vwap_data(df, ValidationLevel.BASIC)
        elif strategy_type == "cumdelta":
            result = StrategyDataValidator.validate_cumdelta_data(df, ValidationLevel.BASIC)
        else:
            result = DataValidator.validate_basic_data(df, ValidationLevel.BASIC)
        
        return result.is_usable
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
        return False


def validate_and_log(df: pd.DataFrame, strategy_name: str = "Unknown",
                    validation_level: ValidationLevel = ValidationLevel.STANDARD) -> ValidationResult:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    result = DataValidator.validate_basic_data(df, validation_level)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    log_level = logging.INFO if result.is_valid else logging.WARNING
    logger.log(log_level, f"[{strategy_name}] {result}")
    
    if result.has_errors:
        for error in result.errors:
            logger.error(f"[{strategy_name}] ‚ùå {error}")
    
    if result.has_warnings:
        for warning in result.warnings:
            logger.warning(f"[{strategy_name}] ‚ö†Ô∏è {warning}")
    
    if result.recommendations:
        for rec in result.recommendations:
            logger.info(f"[{strategy_name}] üí° {rec}")
    
    return result


def get_validation_summary(results: Dict[str, ValidationResult]) -> Dict[str, Any]:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    
    Args:
        results: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å–≤–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    """
    total_datasets = len(results)
    valid_datasets = sum(1 for r in results.values() if r.is_valid)
    usable_datasets = sum(1 for r in results.values() if r.is_usable)
    
    quality_distribution = {}
    for quality in DataQuality:
        count = sum(1 for r in results.values() if r.quality == quality)
        quality_distribution[quality.value] = count
    
    total_errors = sum(len(r.errors) for r in results.values())
    total_warnings = sum(len(r.warnings) for r in results.values())
    
    return {
        "total_datasets": total_datasets,
        "valid_datasets": valid_datasets,
        "usable_datasets": usable_datasets,
        "validation_rate": valid_datasets / total_datasets if total_datasets > 0 else 0,
        "usability_rate": usable_datasets / total_datasets if total_datasets > 0 else 0,
        "quality_distribution": quality_distribution,
        "total_errors": total_errors,
        "total_warnings": total_warnings,
        "overall_quality": "good" if usable_datasets / total_datasets > 0.8 else "poor" if total_datasets > 0 else "unknown"
    }


# =========================================================================
# –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò
# =========================================================================

# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
STRATEGY_MIN_DATA_REQUIREMENTS = {
    "volume_vwap": 100,
    "cumdelta": 80,
    "multitf": 150,
    "scalping": 50,
    "swing": 200
}

# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
CRITICAL_THRESHOLDS = {
    "missing_data_pct": 20.0,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    "outlier_pct": 10.0,       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –≤—ã–±—Ä–æ—Å–æ–≤
    "zero_volume_pct": 15.0,   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –Ω—É–ª–µ–≤—ã—Ö –æ–±—ä–µ–º–æ–≤
    "time_gap_hours": 4.0      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–ø—É—Å–∫ –≤–æ –≤—Ä–µ–º–µ–Ω–∏ (—á–∞—Å—ã)
}

# –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥
VALIDATION_PRESETS = {
    "development": ValidationLevel.BASIC,
    "testing": ValidationLevel.STANDARD,  
    "staging": ValidationLevel.STRICT,
    "production": ValidationLevel.PARANOID
}